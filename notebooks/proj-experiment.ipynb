{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1072389",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from src.conf.NWPaths import NWPaths\n",
    "from conf import catalog, globals\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from collaborative.pipelines.pipelines import Pipelines\n",
    "from dotenv import load_dotenv\n",
    "import mlflow\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7e973ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk-pom:1.10.34,org.apache.hadoop:hadoop-aws:2.7.2 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d41265a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: packages\n",
      "23/05/28 14:42:31 WARN Utils: Your hostname, bruno resolves to a loopback address: 127.0.1.1; using 192.168.1.150 instead (on interface wlp0s20f3)\n",
      "23/05/28 14:42:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/28 14:42:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"Next Watch ML\")\n",
    "    .master(\"local[3]\")\n",
    "    # .config(\"spark.executor.memory\", \"3g\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"96g\")\n",
    "    .config(\"spark.driver.memory\", \"96g\")\n",
    "    .config(\"spark.executor.memory\", \"8g\")\n",
    "    .config(\"packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\")\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34cb1c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.4'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Hadoop version\"\"\"\n",
    "spark._jvm.org.apache.hadoop.util.VersionInfo.getVersion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08f7a1fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8a1a7d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='s3://mlflow/1', creation_time=1685238316335, experiment_id='1', last_update_time=1685238316335, lifecycle_stage='active', name='COLLABORATIVE', tags={}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(os.environ[\"MLFLOW_TRACKING_URI\"])\n",
    "mlflow.set_experiment(globals.MLflow.EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f59a686",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.spark.autolog(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6a68d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = Pipelines(spark, catalog.Sources.MOVIELENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83dcc38e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train, serve = pipelines.data_engineering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f9c3d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13104483",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                  | 0/5 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/28 14:43:57 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/05/28 14:43:58 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "/home/bruno/conda/envs/next-watch/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "\n",
      "Registered model 'spark_als_model' already exists. Creating a new version of this model...\n",
      "2023/05/28 14:46:23 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: spark_als_model, version 22\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|▏| 1/5 [02:44<10:57, 164.40s/trial, best loss: 0.9683697"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '22' of model 'spark_als_model'.\n",
      "Registered model 'spark_als_model' already exists. Creating a new version of this model...\n",
      "2023/05/28 14:49:02 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: spark_als_model, version 23\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|▍| 2/5 [05:22<08:02, 160.93s/trial, best loss: 0.9253851"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '23' of model 'spark_als_model'.\n",
      "Registered model 'spark_als_model' already exists. Creating a new version of this model...\n",
      "2023/05/28 14:51:39 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: spark_als_model, version 24\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|▌| 3/5 [08:00<05:18, 159.41s/trial, best loss: 0.9253851"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '24' of model 'spark_als_model'.\n",
      "Registered model 'spark_als_model' already exists. Creating a new version of this model...\n",
      "2023/05/28 14:54:17 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: spark_als_model, version 25\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|▊| 4/5 [10:38<02:38, 158.88s/trial, best loss: 0.9241907"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '25' of model 'spark_als_model'.\n",
      "Registered model 'spark_als_model' already exists. Creating a new version of this model...\n",
      "2023/05/28 14:56:58 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: spark_als_model, version 26\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█| 5/5 [13:18<00:00, 159.78s/trial, best loss: 0.9241907\n",
      "{'loss': 0.9241907245265494, 'params': {'cold_start_strategy': 'drop', 'max_iter': 5, 'rank': 8, 'reg_param': 0.18636691372197714}, 'status': 'ok'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '26' of model 'spark_als_model'.\n"
     ]
    }
   ],
   "source": [
    "pipelines.data_science(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23ede97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaae3f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
