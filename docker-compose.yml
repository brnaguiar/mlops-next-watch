version: '3.8'

services:

  postgres:
    restart: always
    image: postgres
    container_name: postgres
    ports:
      - "${POSTGRES_PORT}:${POSTGRES_PORT}"
    networks:
      - nwnet
    environment:
      - PGUSER=${POSTGRES_USER}
      - PGPASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_MULTIPLE_DATABASES=${POSTGRES_MLFLOW_DATABASE},${POSTGRES_AIRFLOW_DATABASE}
    volumes:
      - ./docker/postgres/postgres-data:/var/lib/postgresql/data/
    healthcheck:
      test: ["CMD-SHELL", "pg_isready", "-p", "${POSTGRES_PORT}", "-U", "${POSTGRES_USER}"]
      interval: 5s
      timeout: 60s
      retries: 3
  
  create-databases:
    image: postgres
    depends_on:
      - postgres
    networks:
      - nwnet
    environment:
      - PGPASSWORD=${POSTGRES_PASSWORD}
      - PGUSER=${POSTGRES_USER}
      - PGHOST=postgres
    command: >
      bash -c '
      until psql -h postgres -U ${POSTGRES_USER} -c "SELECT 1" > /dev/null 2>&1; do
        sleep 1;
      done;

      psql -h postgres -U ${POSTGRES_USER} -c "ALTER DATABASE template1 REFRESH COLLATION VERSION"; 
      psql -h postgres -U ${POSTGRES_USER} -c "CREATE DATABASE ${POSTGRES_MLFLOW_DATABASE}"; 
      psql -h postgres -U ${POSTGRES_USER} -c "CREATE DATABASE ${POSTGRES_AIRFLOW_DATABASE}"; 
      psql -h postgres -U ${POSTGRES_USER} -c "CREATE DATABASE ${POSTGRES_APP_DATABASE}"; 
      psql -h postgres -U ${POSTGRES_USER} -d ${POSTGRES_APP_DATABASE} -c "CREATE TABLE ${POSTGRES_USERS_TABLE} (userId INTEGER PRIMARY KEY)"; 
      psql -h postgres -U ${POSTGRES_USER} -d ${POSTGRES_APP_DATABASE} -c "DROP TABLE ${POSTGRES_RECOMMENDATIONS_TABLE}";
      psql -h postgres -U ${POSTGRES_USER} -d ${POSTGRES_APP_DATABASE} -c "CREATE TABLE ${POSTGRES_RECOMMENDATIONS_TABLE} (id INTEGER PRIMARY KEY, userid INTEGER, movieid INTEGER, prediction FLOAT, rank INTEGER, CONSTRAINT unique_rank UNIQUE(userid,rank))";
      '

  minio:
    image: minio/minio
    ports:
      - "${MINIO_PORT_SERVER}:${MINIO_PORT_SERVER}"
      - "${MINIO_PORT_CONSOLE}:${MINIO_PORT_CONSOLE}"
    networks:
      - nwnet
    command: >
      server /data 
      --console-address :${MINIO_PORT_CONSOLE} 
      --address :${MINIO_PORT_SERVER}
    environment:  
      - MINIO_ROOT_USER=${MINIO_ACCESS_KEY}
      - MINIO_ROOT_PASSWORD=${MINIO_SECRET_ACCESS_KEY}
      - MINIO_ACCESS_KEY=${MINIO_ACCESS_KEY}
      - MINIO_SECRET_KEY=${MINIO_SECRET_ACCESS_KEY}
      - MINIO_DOMAIN=minio
    volumes:
      - ./docker/minio/minio-data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  create-buckets:
    image: minio/mc:latest
    container_name: minio-create-buckets
    depends_on:
      - minio
    networks:
      - nwnet
    entrypoint: >
      /bin/sh -c '
      sleep 5;
      /usr/bin/mc config host add s3 http://minio:${MINIO_PORT_SERVER} ${MINIO_ACCESS_KEY} ${MINIO_SECRET_ACCESS_KEY} --api S3v4;
      [[ ! -z "`/usr/bin/mc ls s3 | grep ${MLFLOW_BUCKET_NAME}`" ]] || /usr/bin/mc mb s3/${MLFLOW_BUCKET_NAME};
      /usr/bin/mc policy download s3/${MLFLOW_BUCKET_NAME};
      [[ ! -z "`/usr/bin/mc ls s3 | grep ${DATA_BUCKET_NAME}`" ]] || /usr/bin/mc mb s3/${DATA_BUCKET_NAME};
      /usr/bin/mc policy download s3/${DATA_BUCKET_NAME};
      exit 0;
      '

  mlflow:
    restart: always
    build: 
      context: ./
      dockerfile:  ./docker/mlflow/Dockerfile
    image: mlflow-server
    container_name: mlflow-server
    ports:
      - "${MLFLOW_PORT}:${MLFLOW_PORT}" # exposed port : target port:
    networks:
      - nwnet
    environment: 
      - MLFLOW_S3_ENDPOINT_URL=http://minio:${MINIO_PORT_SERVER}
      - AWS_ACCESS_KEY_ID=${MINIO_ACCESS_KEY}
      - AWS_SECRET_ACCESS_KEY=${MINIO_SECRET_ACCESS_KEY}
      - MLFLOW_DFS_TMP=/sparktmp
    command: >
      mlflow server
      --backend-store-uri postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:${POSTGRES_PORT}/${POSTGRES_MLFLOW_DATABASE}
      --host 0.0.0.0
      --default-artifact-root s3a://${MLFLOW_BUCKET_NAME}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/"]
      interval: 30s
      timeout: 10s
      retries: 3

  spark-master:
    image: bitnami/spark:3.4.0
    user: root
    hostname: spark
    container_name: spark
    networks:
      - nwnet
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - DOCKER_RUNNING=true
      - MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI}
      - MLFLOW_DFS_TMP=/sparktmp
      - SPARK_HOME=/opt/bitnami/spark
    volumes:
      - ./src:/app/src
      - ./data:/app/data:rw
      - ./assets:/app/assets
      - ./:/app
      - ./docker/sparktmp:/sparktmp
      #- ./docker/spark/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
    ports: # host port : container port (accessible via localhost : acessible via container IP ip)
      - "8081:8080"
      - "7077:7077"
    deploy:
      resources:
        limits:
          memory: 1GB

  spark-worker:
    image: bitnami/spark:3.4.0
    user: root
    hostname: spark-worker
    container_name: spark-worker
    networks:
        - nwnet
    environment:
        - SPARK_MODE=worker
        - SPARK_MASTER_URL=spark://spark:7077
        - SPARK_WORKER_MEMORY=12G
        - SPARK_WORKER_CORES=2
        - SPARK_RPC_AUTHENTICATION_ENABLED=no
        - SPARK_RPC_ENCRYPTION_ENABLED=no
        - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
        - SPARK_SSL_ENABLED=no
        - DOCKER_RUNNING=true
        - MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI}
        - MLFLOW_DFS_TMP=/sparktmp
        - SPARK_HOME=/opt/bitnami/spark
    volumes:
      - ./src:/app/src
      - ./data:/app/data:rw
      - ./assets:/app/assets
      - ./:/app
      - ./docker/sparktmp:/sparktmp
    depends_on:
        - spark-master
    deploy:
      resources:
        limits:
          memory: 14GB

# Dev env with jupyter
  dev-spark:
    build: 
      context: ./
      dockerfile:  ./docker/dev-spark/Dockerfile
    image: dev-spark
    container_name: dev-spark
    networks:
      - nwnet
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - DOCKER_RUNNING=true
      - MLFLOW_DFS_TMP=/sparktmp
    ports:
      - "8888:8888"
      - "4040-4080:4040-4080"
    volumes:
      - ./:/app:rw
      - ./src:/usr/local/lib/python3.9/site-packages
      - ./src:/usr/local/lib/python3.8/site-packages
      - ./docker/sparktmp:/sparktmp

  fastapi:
    build: 
      context: ./
      dockerfile:  ./docker/frontend/Dockerfile.fastapi
    image: fastapi
    container_name: fastapi
    networks:
      - nwnet
    ports:
      - "8000:8000"
    volumes:
      - ./:/app:r
      - ./data:/app/data:rw

  streamlit:
    build: 
      context: ./
      dockerfile:  ./docker/frontend/Dockerfile.streamlit
    image: streamlit
    container_name: streamlit
    networks:
      - nwnet
    ports:
      - "8501:8501"
    volumes:
      - ./:/app:r

  kafka:
    image: docker.io/bitnami/kafka:3.5.1
    networks:
      - nwnet
    ports:
      - "${KAFKA_PORT}:${KAFKA_PORT}"
    volumes:
      - "kafka-data:/bitnami"
    environment:
      # KRaft settings
      - KAFKA_CFG_NODE_ID=0
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:${KAFKA_PORT_CONTROLLER}
      # Listeners#kafka-topics#kafka-topics#kafka-topics
      - KAFKA_CFG_LISTENERS=PLAINTEXT://${KAFKA_IP}:${KAFKA_PORT},CONTROLLER://${KAFKA_IP}:${KAFKA_PORT_CONTROLLER}
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://${KAFKA_IP}:${KAFKA_PORT}
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT

  init-kafka:
    image: docker.io/bitnami/kafka:3.5.1
    depends_on:  
      - kafka
    entrypoint: [ '/bin/sh', '-c' ]
    command: |
      "
      /opt/bitnami/kafka/bin/kafka-topics.sh --bootstrap-server kafka:${KAFKA_PORT} --list 
      /opt/bitnami/kafka/bin/kafka-topics.sh --bootstrap-server kafka:${KAFKA_PORT} --create --if-not-exists --topic ${KAFKA_RECOMMENDATIONS_TOPIC} --replication-factor 1 --partitions 1
      /opt/bitnami/kafka/bin/kafka-topics.sh --bootstrap-server kafka:${KAFKA_PORT} --create --if-not-exists --topic teste --replication-factor 1 --partitions 1
      echo -e 'Successfully created the following topics:'
      /opt/bitnami/kafka/bin/kafka-topics.sh --bootstrap-server kafka:${KAFKA_PORT} --list
      "
  kafka-pred-consumer:
    build: 
      context: ./
      dockerfile:  ./docker/kafka-pred-consumer/Dockerfile
    image: kafka-pred-consumer
    container_name: kafka-pred-consumer
    networks:
      - nwnet
    volumes:
      - ./:/app:r
    depends_on:
      - postgres
      #- ./src:/usr/local/lib/python3.9/site-packages

volumes:
  mlflow-data:
    driver: local
  minio-data:
    driver: local
  postgres-data:
    driver: local
  kafka-data:
    driver: local
  sparktmp:

networks:
  nwnet:
    driver: bridge

